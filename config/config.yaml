defaults:
   - _self_
   - dataset: cityscapes

crop_size: 400
mean: [0.28689529, 0.32513294, 0.28389176]
std: [0.17613647, 0.18099176, 0.17772235]

train_transforms:
  - _target_: albumentations.SmallestMaxSize
    max_size: ${crop_size}
  - _target_: albumentations.CenterCrop
    height: ${crop_size}
    width: ${crop_size}
  - _target_: albumentations.AdvancedBlur
    p: 0.75
  - _target_: albumentations.GaussNoise
    p: 0.75
  - _target_: albumentations.HorizontalFlip
    p: 0.5
  - _target_: albumentations.CLAHE
    p: 0.75
  - _target_: albumentations.RandomBrightnessContrast
    p: 0.75
  - _target_: albumentations.RandomGamma
    p: 0.75
  - _target_: albumentations.ColorJitter
    p: 0.75
  - _target_: albumentations.Normalize
    mean: ${mean}
    std: ${std}
  - _target_: albumentations.pytorch.ToTensorV2

val_transforms:
  - _target_: albumentations.SmallestMaxSize
    max_size:
  - _target_: albumentations.CenterCrop
    height: ${crop_size}
    width: ${crop_size}
  - _target_: albumentations.Normalize
    mean: ${mean}
    std: ${std}
  - _target_: albumentations.pytorch.ToTensorV2

train_loader:
  _target_: torch.utils.data.DataLoader
  dataset:
    _target_: src.dataloaders.CityscapesDataset
    root: ${dataset.data_dir}
    split: train
    palette: ${dataset.palette}
    transforms: ${train_transforms}
    id_to_train_id: ${dataset.id_to_train_id}
  shuffle: True
  batch_size: 8
  num_workers: 8

val_loader:
  _target_: torch.utils.data.DataLoader
  dataset:
    _target_: src.dataloaders.CityscapesDataset
    root: ${dataset.data_dir}
    split: val
    palette: ${dataset.palette}
    transforms: ${val_transforms}
    id_to_train_id: ${dataset.id_to_train_id}
  batch_size: 8
  num_workers: 4

resume:
